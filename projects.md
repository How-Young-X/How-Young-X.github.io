---
layout: page
permalink: /projects/index.html
title: Projects
---
### Edge Multi-Modal Large Model(Ongoing Project)
We are developing a large model that supports multimodal information input and can be deployed at the edge (e.g., in vehicles). Our design will be based on a Mixture of Experts (MoE) architecture, initially completing model design, training, and quantization on GPUs, followed by FGGA circuit design for deployment on FPGA. This project has just begunâ€”stay tuned!

### How Chain-of-Thought Works ?
This project focuses on LLMs Reasoning Analysis, exploring the mechanisms of Chain-of-Thought(CoT) reasoning. Previous research primarily examined text-level differences and their impact on outputs. This study analyzes model behavior during CoT responses from three perspectives: <strong>decoding</strong>, <strong>activation</strong>, and <strong>projection</strong>. Notably, we found that during the decoding phase, the model effectively mimics and understands CoT prompts, demonstrating increased confidence and specificity in its responses. Compared to standard prompts, CoT prompts activate a broader range of knowledge acquired during pre-training.<br>

## 

<img src="{{ site.url }}/images/" class="bio-photo" alt="{{ site.owner.name }} bio photo" width="100" height="100"></a>
